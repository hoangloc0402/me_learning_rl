{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP =  [\n",
    "    'S-------',\n",
    "    '--------',\n",
    "    '---H----',\n",
    "    '-----H--',\n",
    "    '---H----',\n",
    "    '-HH---H-',\n",
    "    '-H--H-H-',\n",
    "    '---H---G',\n",
    "]\n",
    "MAP_SIZE = (8, 8)\n",
    "MAP_STRING = ''.join(MAP)\n",
    "ACTION_MAPPING = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "\n",
    "ENV = gym.make('FrozenLake8x8-v0')\n",
    "gym.envs.toy_text.frozen_lake.MAPS['8x8'] = MAP\n",
    "ENV = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "\n",
    "ALL_STATE = range(ENV.nS)\n",
    "ALL_ACTION = range(ENV.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_func(V: np.ndarray, precision=2):\n",
    "    rounded = np.round_(V, precision).reshape(MAP_SIZE)\n",
    "    print(' V(s):\\n', rounded, '\\n')\n",
    "\n",
    "\n",
    "def print_policy(policy: np.ndarray):\n",
    "    greedy_policy = np.argmax(policy, axis=1)\n",
    "    what_2_print = list()\n",
    "    for idx, action in enumerate(greedy_policy):\n",
    "        if MAP_STRING[idx] == 'H':\n",
    "            character = '□'\n",
    "        else:\n",
    "            character = ACTION_MAPPING[action]\n",
    "        what_2_print.append(character)\n",
    "\n",
    "    what_2_print = np.array(what_2_print).reshape(MAP_SIZE)\n",
    "    print(' Policy:\\n', what_2_print, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env,\n",
    "                    policy,\n",
    "                    discount_factor = 0.9,\n",
    "                    theta = 1e-6,\n",
    "                    max_iteration = 9999) -> np.ndarray:\n",
    "    print('     POLICY EVALUATION: START!\\n')\n",
    "    # State value function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(1, max_iteration + 1):\n",
    "        is_converged = True\n",
    "\n",
    "        for state in ALL_STATE:\n",
    "            state_value = 0\n",
    "            # For all actions that can be selected by the policy under the current state\n",
    "            for action, action_prob in enumerate(policy[state]):\n",
    "                for state_prob, next_state, reward, terminated in env.P[state][action]:\n",
    "                    state_value += action_prob * state_prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            if abs(V[state] - state_value) > theta:\n",
    "                is_converged = False\n",
    "            V[state] = state_value\n",
    "\n",
    "        if is_converged:\n",
    "            break\n",
    "    print_state_value_func(V)\n",
    "    print(f'     POLICY EVALUATION: finished after ({i}) iterations\\n')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env,\n",
    "                     discount_factor = 0.9,\n",
    "                     max_iteration = 9999) -> (np.ndarray, np.ndarray):\n",
    "    print('POLICY ITERATION: START!\\n')\n",
    "    # Init policy with equal prob for all actions\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    for i in range(1, max_iteration + 1):\n",
    "        is_stable = True\n",
    "        \n",
    "        #First step is evaluating current policy\n",
    "        V = evaluate_policy(env, policy)\n",
    "\n",
    "        for state in ALL_STATE:\n",
    "            current_action = np.argmax(policy[state])\n",
    "            \n",
    "            # See if can find any action that is better than current action\n",
    "            Q = np.zeros(env.nA)\n",
    "            for action in ALL_ACTION:\n",
    "                for prob, next_state, reward, terminated in env.P[state][action]:\n",
    "                    Q[action] += prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            best_action = np.argmax(Q)\n",
    "\n",
    "            if current_action != best_action:\n",
    "                is_stable = False\n",
    "            \n",
    "            # Update current policy greedily\n",
    "            policy[state] = np.zeros(env.nA)\n",
    "            policy[state][best_action] = 1.0\n",
    "            \n",
    "        print_policy(policy)\n",
    "        print('======================================================\\n')\n",
    "\n",
    "        if is_stable:\n",
    "            break    \n",
    "    print(f'POLICY ITERATION: finished after ({i}) iterations\\n')\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLICY ITERATION: START!\n",
      "\n",
      "     POLICY EVALUATION: START!\n",
      "\n",
      " V(s):\n",
      " [[0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.01]\n",
      " [0.   0.   0.   0.   0.   0.   0.01 0.02]\n",
      " [0.   0.   0.   0.   0.   0.01 0.01 0.04]\n",
      " [0.   0.   0.   0.   0.   0.01 0.   0.12]\n",
      " [0.   0.   0.   0.   0.   0.03 0.   0.36]\n",
      " [0.   0.   0.   0.   0.04 0.12 0.36 0.  ]] \n",
      "\n",
      "     POLICY EVALUATION: finished after (35) iterations\n",
      "\n",
      " Policy:\n",
      " [['→' '→' '→' '→' '→' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '→' '→' '↓' '↓']\n",
      " ['→' '↑' '↑' '□' '→' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '→' '↓']\n",
      " ['↑' '↑' '↑' '□' '→' '→' '→' '↓']\n",
      " ['↑' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↑' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n",
      "======================================================\n",
      "\n",
      "     POLICY EVALUATION: START!\n",
      "\n",
      " V(s):\n",
      " [[0.25 0.28 0.31 0.35 0.39 0.43 0.48 0.53]\n",
      " [0.28 0.31 0.35 0.39 0.43 0.48 0.53 0.59]\n",
      " [0.25 0.28 0.31 0.   0.48 0.53 0.59 0.66]\n",
      " [0.35 0.39 0.43 0.48 0.53 0.   0.66 0.73]\n",
      " [0.31 0.35 0.39 0.   0.59 0.66 0.73 0.81]\n",
      " [0.28 0.   0.   0.59 0.66 0.73 0.   0.9 ]\n",
      " [0.25 0.   0.48 0.53 0.   0.81 0.   1.  ]\n",
      " [0.35 0.39 0.43 0.   0.81 0.9  1.   0.  ]] \n",
      "\n",
      "     POLICY EVALUATION: finished after (15) iterations\n",
      "\n",
      " Policy:\n",
      " [['↓' '↓' '↓' '↓' '↓' '↓' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '□' '↓' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '↓' '↓']\n",
      " ['→' '→' '↑' '□' '↓' '↓' '→' '↓']\n",
      " ['↑' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↓' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n",
      "======================================================\n",
      "\n",
      "     POLICY EVALUATION: START!\n",
      "\n",
      " V(s):\n",
      " [[0.25 0.28 0.31 0.35 0.39 0.43 0.48 0.53]\n",
      " [0.28 0.31 0.35 0.39 0.43 0.48 0.53 0.59]\n",
      " [0.31 0.35 0.39 0.   0.48 0.53 0.59 0.66]\n",
      " [0.35 0.39 0.43 0.48 0.53 0.   0.66 0.73]\n",
      " [0.31 0.35 0.39 0.   0.59 0.66 0.73 0.81]\n",
      " [0.28 0.   0.   0.59 0.66 0.73 0.   0.9 ]\n",
      " [0.31 0.   0.48 0.53 0.   0.81 0.   1.  ]\n",
      " [0.35 0.39 0.43 0.   0.81 0.9  1.   0.  ]] \n",
      "\n",
      "     POLICY EVALUATION: finished after (15) iterations\n",
      "\n",
      " Policy:\n",
      " [['↓' '↓' '↓' '↓' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '□' '↓' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '↓' '↓']\n",
      " ['→' '→' '↑' '□' '↓' '↓' '→' '↓']\n",
      " ['↓' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↓' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n",
      "======================================================\n",
      "\n",
      "     POLICY EVALUATION: START!\n",
      "\n",
      " V(s):\n",
      " [[0.25 0.28 0.31 0.35 0.39 0.43 0.48 0.53]\n",
      " [0.28 0.31 0.35 0.39 0.43 0.48 0.53 0.59]\n",
      " [0.31 0.35 0.39 0.   0.48 0.53 0.59 0.66]\n",
      " [0.35 0.39 0.43 0.48 0.53 0.   0.66 0.73]\n",
      " [0.31 0.35 0.39 0.   0.59 0.66 0.73 0.81]\n",
      " [0.28 0.   0.   0.59 0.66 0.73 0.   0.9 ]\n",
      " [0.31 0.   0.48 0.53 0.   0.81 0.   1.  ]\n",
      " [0.35 0.39 0.43 0.   0.81 0.9  1.   0.  ]] \n",
      "\n",
      "     POLICY EVALUATION: finished after (15) iterations\n",
      "\n",
      " Policy:\n",
      " [['↓' '↓' '↓' '↓' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '□' '↓' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '↓' '↓']\n",
      " ['→' '→' '↑' '□' '↓' '↓' '→' '↓']\n",
      " ['↓' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↓' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n",
      "======================================================\n",
      "\n",
      "POLICY ITERATION: finished after (4) iterations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, V = policy_iteration(ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
