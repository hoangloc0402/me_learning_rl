{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP =  [\n",
    "    'S-------',\n",
    "    '--------',\n",
    "    '---H----',\n",
    "    '-----H--',\n",
    "    '---H----',\n",
    "    '-HH---H-',\n",
    "    '-H--H-H-',\n",
    "    '---H---G',\n",
    "]\n",
    "MAP_SIZE = (8, 8)\n",
    "MAP_STRING = ''.join(MAP)\n",
    "ACTION_MAPPING = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "\n",
    "ENV = gym.make('FrozenLake8x8-v0')\n",
    "gym.envs.toy_text.frozen_lake.MAPS['8x8'] = MAP\n",
    "ENV = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "\n",
    "ALL_STATE = range(ENV.nS)\n",
    "ALL_ACTION = range(ENV.nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_func(V: np.ndarray, precision=2):\n",
    "    rounded = np.round_(V, precision).reshape(MAP_SIZE)\n",
    "    print(' V(s):\\n', rounded, '\\n')\n",
    "\n",
    "\n",
    "def print_policy(policy: np.ndarray):\n",
    "    greedy_policy = np.argmax(policy, axis=1)\n",
    "    what_2_print = list()\n",
    "    for idx, action in enumerate(greedy_policy):\n",
    "        if MAP_STRING[idx] == 'H':\n",
    "            character = '□'\n",
    "        else:\n",
    "            character = ACTION_MAPPING[action]\n",
    "        what_2_print.append(character)\n",
    "\n",
    "    what_2_print = np.array(what_2_print).reshape(MAP_SIZE)\n",
    "    print(' Policy:\\n', what_2_print, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value_func(env,\n",
    "                             discount_factor = 0.95,\n",
    "                             theta = 1e-9,\n",
    "                             max_iteration = 9999) -> np.ndarray:\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    for i in range(1, max_iteration + 1):\n",
    "        is_converged = True\n",
    "        # For every state\n",
    "        for state in ALL_STATE:\n",
    "            # Compute the state action value Q(s, a) for all actions in that state\n",
    "            Q = np.zeros(env.nA)\n",
    "            for action in ALL_ACTION:\n",
    "                # For an action, there can be multiple posible next state\n",
    "                for prob, next_state, reward, terminated in env.P[state][action]:\n",
    "                    Q[action] += prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            old_state_value = V[state]\n",
    "            V[state]= np.max(Q)\n",
    "\n",
    "            if abs(old_state_value - V[state]) > theta:\n",
    "                is_converged = False\n",
    "\n",
    "        if is_converged:\n",
    "            break\n",
    "            \n",
    "    print(f'COMPUTING V(s): finished after {i} iterations\\n')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(env,\n",
    "                  V: np.ndarray,\n",
    "                  discount_factor = 0.9) -> np.ndarray:\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    # For every state\n",
    "    for state in range(env.nS):\n",
    "        # Compute the state action value Q(s, a) for all actions in that state\n",
    "        Q = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            # For an action, there can be multiple posible next state\n",
    "            for prob, next_state, reward, terminated in env.P[state][action]:\n",
    "                Q[action] += prob * (reward + discount_factor * V[next_state])\n",
    "        best_action = np.argmax(Q)\n",
    "        policy[state][best_action] = 1.0\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING V(s): finished after 15 iterations\n",
      "\n",
      " V(s):\n",
      " [[0.51 0.54 0.57 0.6  0.63 0.66 0.7  0.74]\n",
      " [0.54 0.57 0.6  0.63 0.66 0.7  0.74 0.77]\n",
      " [0.57 0.6  0.63 0.   0.7  0.74 0.77 0.81]\n",
      " [0.6  0.63 0.66 0.7  0.74 0.   0.81 0.86]\n",
      " [0.57 0.6  0.63 0.   0.77 0.81 0.86 0.9 ]\n",
      " [0.54 0.   0.   0.77 0.81 0.86 0.   0.95]\n",
      " [0.57 0.   0.7  0.74 0.   0.9  0.   1.  ]\n",
      " [0.6  0.63 0.66 0.   0.9  0.95 1.   0.  ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = compute_state_value_func(ENV)\n",
    "print_state_value_func(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Policy:\n",
      " [['↓' '↓' '↓' '↓' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '□' '↓' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '↓' '↓']\n",
      " ['→' '→' '↑' '□' '↓' '↓' '→' '↓']\n",
      " ['↓' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↓' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy = create_policy(ENV, V)\n",
    "print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
