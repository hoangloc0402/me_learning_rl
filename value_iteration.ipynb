{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.envs.toy_text import frozen_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make('FrozenLake8x8-v0')\n",
    "MAP =  ['S-------',\n",
    "        '--------',\n",
    "        '---H----',\n",
    "        '-----H--',\n",
    "        '---H----',\n",
    "        '-HH---H-',\n",
    "        '-H--H-H-',\n",
    "        '---H---G']\n",
    "frozen_lake.MAPS['8x8'] = MAP\n",
    "ENV = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "\n",
    "ACTION_MAPPING = { 0: '←', 1: '↓', 2: '→', 3: '↑'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_state_value_func(gym_env,\n",
    "                             discount_factor = 0.99,\n",
    "                             eps = 1e-9,\n",
    "                             max_iter = 9999) -> np.ndarray:\n",
    "    V = np.zeros(gym_env.nS)\n",
    "\n",
    "    for i in range(1, max_iter + 1):\n",
    "        is_converged = True\n",
    "        # For every state\n",
    "        for state in range(gym_env.nS):\n",
    "            # Compute the state action value Q(s, a) for all actions in that state\n",
    "            Q = np.zeros(gym_env.nA)\n",
    "            for action in range(gym_env.nA):\n",
    "                # For an action, there can be multiple posible next state\n",
    "                for prob, next_state, reward, terminated in gym_env.P[state][action]:\n",
    "                    Q[action] += prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            old_state_value = V[state]\n",
    "            V[state]= np.max(Q)\n",
    "\n",
    "            if abs(old_state_value - V[state]) > eps:\n",
    "                is_converged = False\n",
    "\n",
    "        if is_converged:\n",
    "            print(f'COMPUTING V(s): converged after {i} iterations\\n')\n",
    "            return V\n",
    "\n",
    "    print(f'COMPUTING V(s): reached max number of iterations ({max_iter})\\n')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(gym_env,\n",
    "                  V: np.ndarray,\n",
    "                  discount_factor = 0.99) -> np.ndarray:\n",
    "    policy = np.zeros([gym_env.nS, gym_env.nA])\n",
    "    # For every state\n",
    "    for state in range(gym_env.nS):\n",
    "        # Compute the state action value Q(s, a) for all actions in that state\n",
    "        Q = np.zeros(gym_env.nA)\n",
    "        for action in range(gym_env.nA):\n",
    "            # For an action, there can be multiple posible next state\n",
    "            for prob, next_state, reward, terminated in gym_env.P[state][action]:\n",
    "                Q[action] += prob * (reward + discount_factor * V[next_state])\n",
    "        best_action = np.argmax(Q)\n",
    "        policy[state] = np.eye(gym_env.nA)[best_action]\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_func(V):\n",
    "    print(' V(s):')\n",
    "    print(np.round_(V, 2).reshape(8, 8), '\\n')\n",
    "\n",
    "def print_policy(policy: np.ndarray):\n",
    "    print(' POLICY: ')\n",
    "    temp_policy = np.argmax(policy, axis = 1)\n",
    "    temp_map = ''.join(MAP)\n",
    "    string_map = list()\n",
    "    for idx, action in enumerate(temp_policy):\n",
    "        if temp_map[idx] == 'H':\n",
    "            string_map.append('□')\n",
    "        else:\n",
    "            string_map.append(ACTION_MAPPING[action])\n",
    "    string_map = np.array(string_map).reshape((8, 8))\n",
    "    print(string_map, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTING V(s): converged after 15 iterations\n",
      "\n",
      " V(s):\n",
      "[[0.88 0.89 0.9  0.9  0.91 0.92 0.93 0.94]\n",
      " [0.89 0.9  0.9  0.91 0.92 0.93 0.94 0.95]\n",
      " [0.9  0.9  0.91 0.   0.93 0.94 0.95 0.96]\n",
      " [0.9  0.91 0.92 0.93 0.94 0.   0.96 0.97]\n",
      " [0.9  0.9  0.91 0.   0.95 0.96 0.97 0.98]\n",
      " [0.89 0.   0.   0.95 0.96 0.97 0.   0.99]\n",
      " [0.9  0.   0.93 0.94 0.   0.98 0.   1.  ]\n",
      " [0.9  0.91 0.92 0.   0.98 0.99 1.   0.  ]] \n",
      "\n",
      " POLICY: \n",
      "[['↓' '↓' '↓' '↓' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '□' '↓' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '↓' '↓']\n",
      " ['→' '→' '↑' '□' '↓' '↓' '→' '↓']\n",
      " ['↓' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↓' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = compute_state_value_func(ENV)\n",
    "print_state_value_func(V)\n",
    "\n",
    "policy = create_policy(ENV, V)\n",
    "print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(gym_env, policy, num_episodes):\n",
    "    total_win, total_reward = 0, 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = gym_env.reset()\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            action = np.argmax(policy[state])\n",
    "            next_state, reward, terminated, info = gym_env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if terminated and reward == 1.0:\n",
    "                total_win += 1\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    \n",
    "    print(f'NUM_EPISODES: {num_episodes}')\n",
    "    print(f'   TOTAL_WIN: {total_win}')\n",
    "    print(f'  AVG REWARD: {avg_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPISODES: 1000\n",
      "   TOTAL_WIN: 1000\n",
      "  AVG REWARD: 1.0\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 1000\n",
    "\n",
    "play(ENV, policy, NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
