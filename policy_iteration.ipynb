{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.envs.toy_text import frozen_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make('FrozenLake8x8-v0')\n",
    "MAP =  ['S-------',\n",
    "        '--------',\n",
    "        '---H----',\n",
    "        '-----H--',\n",
    "        '---H----',\n",
    "        '-HH---H-',\n",
    "        '-H--H-H-',\n",
    "        '---H---G']\n",
    "frozen_lake.MAPS['8x8'] = MAP\n",
    "ENV = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "\n",
    "ACTION_MAPPING = { 0: '←', 1: '↓', 2: '→', 3: '↑'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(gym_env,\n",
    "                    policy,\n",
    "                    discount_factor = 0.9,\n",
    "                    eps = 1e-6,\n",
    "                    max_iter = 9999) -> np.ndarray:\n",
    "    \n",
    "    V = np.zeros(gym_env.nS)\n",
    "\n",
    "    for i in range(1, max_iter + 1):\n",
    "        is_converged = True\n",
    "        # For all state\n",
    "        for state in range(gym_env.nS):\n",
    "            state_value = 0\n",
    "            # For all actions that can be selected by the policy under the current state\n",
    "            for action, action_prob in enumerate(policy[state]):\n",
    "                for state_prob, next_state, reward, terminated in gym_env.P[state][action]:\n",
    "                    state_value += action_prob * state_prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "                if abs(V[state] - state_value) > eps:\n",
    "                    is_converged = False\n",
    "                V[state] = state_value\n",
    "\n",
    "        if is_converged:\n",
    "            print(f'POLICY EVALUATION: V(s) converged after {i} iterations\\n')\n",
    "            return V\n",
    "\n",
    "    print(f'POLICY EVALUATION: reached max number of iterations ({max_iter})\\n')\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gym_env,\n",
    "                     discount_factor = 0.9,\n",
    "                     max_iter = 9999) -> (np.ndarray, np.ndarray):\n",
    "    \n",
    "    # Init policy with equal prob for all actions\n",
    "    policy = np.ones([gym_env.nS, gym_env.nA]) / gym_env.nA\n",
    "    \n",
    "    for i in range(1, max_iter + 1):\n",
    "        is_stable = True\n",
    "        #First step is evaluating current policy\n",
    "        V = evaluate_policy(gym_env, policy)\n",
    "        \n",
    "        for state in range(gym_env.nS):\n",
    "            current_action = np.argmax(policy[state])\n",
    "            \n",
    "            # See if can find any action that is better than current action\n",
    "            Q = np.zeros(gym_env.nA)\n",
    "            for action in range(gym_env.nA):\n",
    "                for prob, next_state, reward, terminated in gym_env.P[state][action]:\n",
    "                    Q[action] += prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            best_action = np.argmax(Q)\n",
    "\n",
    "            if current_action != best_action:\n",
    "                is_stable = False\n",
    "            \n",
    "            # Update current policy greedily\n",
    "            policy[state] = np.zeros(gym_env.nA)\n",
    "            policy[state][best_action] = 1.0\n",
    "        \n",
    "        if is_stable:\n",
    "            print(f'Policy converged after {i} iterations\\n')\n",
    "            return policy, V\n",
    "    \n",
    "    print(f'Policy iteration reached max number of iterations ({max_iter})\\n')\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_func(V):\n",
    "    print(' V(s):')\n",
    "    print(np.round_(V, 2).reshape(8, 8), '\\n')\n",
    "\n",
    "def print_policy(policy: np.ndarray):\n",
    "    print(' POLICY: ')\n",
    "    temp_policy = np.argmax(policy, axis = 1)\n",
    "    temp_map = ''.join(MAP)\n",
    "    string_map = list()\n",
    "    for idx, action in enumerate(temp_policy):\n",
    "        if temp_map[idx] == 'H':\n",
    "            string_map.append('□')\n",
    "        else:\n",
    "            string_map.append(ACTION_MAPPING[action])\n",
    "    string_map = np.array(string_map).reshape((8, 8))\n",
    "    print(string_map, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLICY EVALUATION: reached max number of iterations (9999)\n",
      "\n",
      "POLICY EVALUATION: reached max number of iterations (9999)\n",
      "\n",
      "POLICY EVALUATION: reached max number of iterations (9999)\n",
      "\n",
      "POLICY EVALUATION: reached max number of iterations (9999)\n",
      "\n",
      "Policy converged after 4 iterations\n",
      "\n",
      " V(s):\n",
      "[[0.25 0.28 0.31 0.35 0.39 0.43 0.48 0.53]\n",
      " [0.28 0.31 0.35 0.39 0.43 0.48 0.53 0.59]\n",
      " [0.31 0.35 0.39 0.   0.48 0.53 0.59 0.66]\n",
      " [0.35 0.39 0.43 0.48 0.53 0.   0.66 0.73]\n",
      " [0.31 0.35 0.39 0.   0.59 0.66 0.73 0.81]\n",
      " [0.28 0.   0.   0.59 0.66 0.73 0.   0.9 ]\n",
      " [0.31 0.   0.48 0.53 0.   0.81 0.   1.  ]\n",
      " [0.35 0.39 0.43 0.   0.81 0.9  1.   0.  ]] \n",
      "\n",
      " POLICY: \n",
      "[['↓' '↓' '↓' '↓' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '→' '↓' '↓' '↓' '↓']\n",
      " ['↓' '↓' '↓' '□' '↓' '→' '↓' '↓']\n",
      " ['→' '→' '→' '→' '↓' '□' '↓' '↓']\n",
      " ['→' '→' '↑' '□' '↓' '↓' '→' '↓']\n",
      " ['↓' '□' '□' '→' '→' '↓' '□' '↓']\n",
      " ['↓' '□' '→' '↑' '□' '↓' '□' '↓']\n",
      " ['→' '→' '↑' '□' '→' '→' '→' '←']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, V = policy_iteration(ENV)\n",
    "\n",
    "print_state_value_func(V)\n",
    "print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(gym_env, policy, num_episodes):\n",
    "    total_win, total_reward = 0, 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = gym_env.reset()\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            action = np.argmax(policy[state])\n",
    "            next_state, reward, terminated, info = gym_env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if terminated and reward == 1.0:\n",
    "                total_win += 1\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    \n",
    "    print(f'NUM_EPISODES: {num_episodes}')\n",
    "    print(f'   TOTAL_WIN: {total_win}')\n",
    "    print(f'  AVG REWARD: {avg_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_EPISODES: 1000\n",
      "   TOTAL_WIN: 1000\n",
      "  AVG REWARD: 1.0\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 1000\n",
    "\n",
    "play(ENV, policy, NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
